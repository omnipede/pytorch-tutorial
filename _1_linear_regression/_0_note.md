# 선형 회귀

## 선형 회귀란

입력 x 와 출력 y 의 선형 상관 관계를 모델링하는 기법이다. 
예를 들어 100원을 넣었을 때 음료수가 2캔, 200원을 넣었을 때 4 캔, 300원을 넣었을 때 6캔이 나오는 자판기가 있다고 가정하자.
이 때, 400원을 넣는다면 음료수가 몇 캔 나올지를 예측하는 것이 선형 회귀라고 러프하게 말할 수 있다.

## Training 이란

앞선 자판기 예제에서 400원을 넣었을 때 음료수가 몇 캔 나올지 예측하기 위해서는 얼마를 넣었을 때 얼마가 나왔다는 데이터가 있어야 한다. 
이 데이터를 ```training dataset``` 이라고 한다. 그리고 금액에 따른 획득 캔 수를 예측하는 모델을 ```가설``` 이라고 한다.  
선형 회귀 문제에서는 주로

```markdown
H(x) = Wx + b
```

로 가설을 수립힌다. 그리고 가설에 훈련 데이터를 대입해보고, 훈련 데이터 xi 에 대한 실제 출력값 (yi) 과 가설 결과값 (H(xi))을 비교하는 식을 ```비용함수``` 라고 한다. 
선형 회귀 문제에서는 주로

```markdown
cost(W, b) = mse(yi, H(xi))
```

로 설정한다. 여기서 mse 는 mean squared error 함수다.

비용함수 ```cost(W, b)``` 의 값이 최소화되는 ```W, b``` 찾는 과정을 ```학습``` 이라고 하고 
이 때, ```W, b``` 를 찾는 알고리즘을 ```Optimizer``` 알고리즘이라고 한다.

## 경사하강법 (Gradient Descent)

선형 회귀 문제에서 주로 사용하는 optimizer 는 경사하강법이다. 
식을 단순화 하기 위해

```markdown
y = Wx
```

로 하면 비용함수 ```cost(W)``` 는 아래와 같이 이차 함수 형태를 갖는다.

![스크린샷 2022-02-04 오후 2 37 56](https://user-images.githubusercontent.com/41066039/152478126-372ba47b-5429-435a-9c77-c785a5afab22.png)

가설에 대한 오차를 최소화하기 위해서는 이차 함수에서 미분했을 때 기울기가 0 인 지점의 W 를 찾아야 한다.
이 지점을 찾기 위해 경사 하강법에서는 특정 지점의 기울기를 구한 뒤, 기울기 값에 작은 크기의 상수를 곱한 값만큼 W 값에서 빼는 식으로 W 값을 조정한다.
여기서 기울기에 곱하는 상수 값을 ```learning rate``` 라고 한다.

## Pytorch 실습

* [대표 예제](./_1_representative.py)
  
* [자동 미분](./_2_autograd.py)

* [Multivalue 선형 회귀](./_3_multivalue.py)

* [nn 모듈을 이용한 방법](./_4_using_nn_module.py)

* [nn 모듈을 상속받은 클래스를 이용한 방법](./_5_using_nn_class.py)
